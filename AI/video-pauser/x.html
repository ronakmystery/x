<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Class Detection + MediaPipe Hands</title>

  <!-- TensorFlow.js + Teachable Machine Image Library -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>

  <!-- MediaPipe Hands and Camera Utils -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils"></script>
  
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      background-color: #f4f4f9;
      margin: 0;
      padding: 20px;
    }
    #video-container {
      margin: 20px auto;
      position: relative;
    }
    video, #webcam {
      width: 1280px;
      height: 720px;
      border-radius: 10px;
      border: 2px solid #333;
    }
    #webcam{
        display: none;
    }
    #label-container div {
      margin-top: 10px;
      font-size: 18px;
      font-weight: bold;
    }
    .status {
      margin-top: 20px;
      font-size: 18px;
      font-weight: bold;
      color: #333;
    }
    
    canvas {
        position: absolute;
    top: 0;
    left: 0;
    right: 0;
    margin: auto;
        }
  </style>
</head>
<body>
  <h1>Class Detection + Gesture</h1>
  <p>
    pause video if class detected and show class from webcam to unpause  <br/>
    peace sign to play <br/>
    fist to pause
  </p>

  <!-- Video Upload -->
  <input type="file" id="videoUpload" accept="video/*" />

  <!-- Video & Webcam Container -->
  <div id="video-container">
    <video id="videoPlayer" controls></video>
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="overlayCanvas" width="1280" height="720"></canvas>

  </div>

  <!-- Class Prediction Labels -->
  <div id="label-container"></div>

  

  <script>
    // ====== Teachable Machine Model Setup ======
    const URL = "./model/"; // Path to your Teachable Machine model folder
    let model, labelContainer, maxPredictions;
    let videoPlayer = document.getElementById("videoPlayer");
    let webcamElement = document.getElementById("webcam");

    // Track whether class  was detected (which triggers pause)
    let currentClass = "";
    let detected = false; // If class currently detected

    const ctx = overlayCanvas.getContext('2d');


    async function initModel() {
      const modelURL = URL + "model.json";
      const metadataURL = URL + "metadata.json";
      model = await tmImage.load(modelURL, metadataURL);
      maxPredictions = model.getTotalClasses();

      labelContainer = document.getElementById("label-container");
      labelContainer.innerHTML = "";
      classNames=["frog","water bottle",]
      for (let i = 0; i < maxPredictions; i++) {
        labelContainer.appendChild(document.createElement("div"));
      }
      console.log("Teachable Machine model loaded.");
    }

    // ====== Webcam Setup ======
    let webcamStream;
    async function setupWebcam() {
      webcamStream = await navigator.mediaDevices.getUserMedia({ video: true });
      webcamElement.srcObject = webcamStream;
      return new Promise((resolve) => {
        webcamElement.onloadedmetadata = () => resolve(webcamElement);
      });
    }

    // ====== Predict on Video Frames (Teachable Machine) ======
    async function predictVideo() {
      if (!model) return; // Ensure model is loaded
      const prediction = await model.predict(videoPlayer);
      for (let i = 0; i < maxPredictions; i++) {
      

        // If Class A or B â‰¥ 0.9 confidence => pause video
        if (
          (prediction[i].className === "A" || prediction[i].className === "B") &&
          prediction[i].probability >= 0.9 
        ) {
          videoPlayer.pause();
          currentClass = prediction[i].className;
          detected = true;
          break; // Stop checking once we know it's paused
        } else {
          // If not A or B with required confidence, no detection
          detected = false;
        }
      }
    }

    // ====== Predict on Webcam Frames (Teachable Machine) ======
    // (If you also want TM-based unpausing on same class, keep this code. 
    //  If you only want to rely on the peace sign for unpausing, you can remove it.)
    async function predictWebcam() {
      if (!model) return; // Ensure model is loaded
      const prediction = await model.predict(webcamElement);
      
      for (let i = 0; i < maxPredictions; i++) {
        const classPrediction =
          classNames[i] + ": " + Math.round(prediction[i].probability * 100)
          +"%";
          
        labelContainer.childNodes[i].innerHTML = classPrediction;
        if (
          prediction[i].className === currentClass &&
          prediction[i].probability >= 0.9 &&
          detected === true
        ) {
          videoPlayer.play(); // Unpause if we see the same class in the webcam
          console.log("Detected same class in webcam, resuming video.");
          break;
        }
      }
    }

    // ====== MediaPipe Hands Setup ======
    let handsInitialized = false;
    const hands = new Hands({
      locateFile: (file) =>
        `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });

    // Callback: fired each time Hands has a new result
    hands.onResults(onHandResults);

    function onHandResults(results) {

        
      const gestureStatus = document.getElementById("gesture-status");

      // If no hands detected, set text to None
      if (!results.multiHandLandmarks || results.multiHandLandmarks.length === 0) {
        return;
      }

      // Check the first detected hand
      const landmarks = results.multiHandLandmarks[0];
      ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

         // Draw landmarks
         landmarks.forEach((landmark) => {
            const x = landmark.x * overlayCanvas.width;
            const y = landmark.y * overlayCanvas.height;

            ctx.beginPath();
            ctx.arc(x, y, 5, 0, 2 * Math.PI);
            ctx.fillStyle = 'red';
            ctx.fill();
        });

      // Indices for tips of each finger: thumb=4, index=8, middle=12, ring=16, pinky=20
      const fingerTips = [4, 8, 12, 16, 20];
      let extendedFingers = 0;
      const extendedNames = [];

      fingerTips.forEach((tip, i) => {
        const tipLandmark = landmarks[tip];
        // For each finger, compare tip landmark to lower joint
        // (Thumb uses a slightly different approach, but let's keep it simple.)
        // We'll assume if tip y < joint y => finger is extended
        let baseIdx = tip - (i === 0 ? 1 : 2); // thumb vs others
        const baseLandmark = landmarks[baseIdx];

        // Check extension
        if (i === 0) {
          // Rough approach for thumb: compare x-coordinates
          if (tipLandmark.x < baseLandmark.x) {
            extendedFingers++;
            extendedNames.push("thumb");
          }
        } else {
          // Others: compare y-coordinates
          if (tipLandmark.y < baseLandmark.y) {
            extendedFingers++;
            if (i === 1) extendedNames.push("index");
            if (i === 2) extendedNames.push("middle");
            if (i === 3) extendedNames.push("ring");
            if (i === 4) extendedNames.push("pinky");
          }
        }
        // console.log(extendedNames)

      });

      // Simple logic: if exactly 2 extended fingers and they are index+middle => Peace
      if(extendedFingers<=1){
        if (!videoPlayer.paused) {
                videoPlayer.pause();
                console.log("Fist detected. Video paused.");
            }

            ctx.fillStyle = 'red';
                ctx.font = '50px Arial';
                ctx.fillText(`Pause`, 50, 100);
      }else if (
        extendedFingers <=3 &&
        extendedNames.includes("index") &&
        extendedNames.includes("middle")
      ) {
        // If the video is paused, let's unpause
        if (videoPlayer.paused) {
          videoPlayer.play();
          ctx.fillStyle = 'red';
                ctx.font = '50px Arial';
                ctx.fillText(`Play`, 50, 100);
          console.log("Peace sign detected, video playing...");
        }
      }
    }

    // Initialize the MediaPipe Camera (for the same <video> webcamElement)
    function setupHandsCamera() {
      const camera = new Camera(webcamElement, {
        onFrame: async () => {
          // Only send frames if hands is ready
          if (handsInitialized) {
            await hands.send({ image: webcamElement });
          }
        },
        width: 1280,
        height: 720
      });
      camera.start();
      handsInitialized = true;
    }

    // ====== Main Flow ======
    const videoUpload = document.getElementById("videoUpload");

    videoUpload.addEventListener("change", async (event) => {
      const file = event.target.files[0];
      if (file) {
        const reader = new FileReader();
        reader.onload = function (e) {
          videoPlayer.src = e.target.result;
          videoPlayer.play();
        };
        reader.readAsDataURL(file);

        // Load the TM model if not already loaded
        if (!model) {
          await initModel();
        }
        // Setup the webcam if not already done
        if (!webcamStream) {
          await setupWebcam();
          setupHandsCamera(); // Also init MediaPipe Hands on the same webcam
        }
      }
    });

    // Each frame of video => check if it has Class A/B
    videoPlayer.addEventListener("timeupdate", async () => {
      if (videoPlayer.readyState === videoPlayer.HAVE_ENOUGH_DATA) {
        await predictVideo();
      }
    });

    // Periodically check the webcam for the same class or run other logic
    webcamElement.addEventListener("play", () => {
      setInterval(async () => {
        if (!webcamElement.paused && webcamElement.readyState === 4) {
          await predictWebcam(); // If we want to unpause when the same class is found
          // Meanwhile, onFrame for MediaPipe is handled automatically by camera_utils
        }
      }, 100);
    });
  </script>
</body>
</html>
